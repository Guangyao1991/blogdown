---
title: "岭回归(Ridge regression)"
author: "Guangyao Zhao"
date: '2022-05-01T21:09:12-05:00'
image: ML.jpg
categories: machine learning
tags:
- machine learning
- algorithm
---



<p>当样本数较小的时候，模型容易过拟合，此时一般有三种办法解决：</p>
<ol style="list-style-type: decimal">
<li>增加样本量</li>
<li>提取出有效特征，减少维度</li>
<li>正则化</li>
</ol>
<p>线性回归中，<span class="math inline">\(\mathrm{L}_2\)</span>正则的代价函数如下：</p>
<p><span class="math display">\[
\mathrm{J}(w) =\sum_{i=1}^{n}\Vert w^\mathrm{T}x_i-y_i\Vert^2 +\lambda w^\mathrm{T}w
\]</span></p>
<p>在线性回归一节中，推导出：</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{J}(w)&amp;=\left ( w^\mathrm{T}\mathbf{X}^\mathrm{T}-y^\mathrm{T} \right ) \left ( \mathbf{X}w-y \right )\\
&amp; = w^\mathrm{T} \mathbf{X}^\mathrm{T} \mathbf{X}w - 2w^\mathrm{T}\mathbf{X}^\mathrm{T}y+y^\mathrm{T}y
\end{aligned}
\]</span></p>
<p>代入包含<span class="math inline">\(\mathrm{L}_2\)</span>正则的代价函数中：</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{J}(w)&amp; = w^\mathrm{T} \mathbf{X}^\mathrm{T} \mathbf{X}w - 2w^\mathrm{T}\mathbf{X}^\mathrm{T}y+y^\mathrm{T}y+\lambda w^\mathrm{T}w\\
&amp;= w^\mathrm{T}\left ( \mathbf{X}^\mathrm{T}\mathbf{X}+\lambda \mathrm{I} \right ) w - 2w^\mathrm{T}\mathbf{X}^\mathrm{T}y+y^\mathrm{T}y
\end{aligned}
\]</span></p>
<p>对上式求导得到：</p>
<p><span class="math display">\[
\frac{\partial\, \mathrm{J}(w)}{\partial\, w} = 2\left (\mathbf{X} ^\mathrm{T}\mathbf{X}+\lambda\mathrm{I}  \right ) w-2\mathbf{X}^\mathrm{T}y
\]</span></p>
<p>求解可得到解析解：<span class="math inline">\(w=\left ( \mathbf{X}^\mathrm{T}\mathbf{X} +\lambda\mathrm{I}\right )^{-1}\mathbf{X}^\mathrm{T}y\)</span>。在进行梯度下降的时候也是利用该公式，<span class="math inline">\(w\)</span>的更新公式为：</p>
<p><span class="math display">\[
w := w-\left(\alpha \mathbf{X} ^\mathrm{T}\left( \mathbf{X}w-y\right)-\lambda w\right)
\]</span></p>
<p>非矩阵形式的更新公式如下：</p>
<p><span class="math display">\[
w_j := w_j - \left(\alpha\frac{1}{n}\sum_{i=1}^{n}(w^\mathrm{T}x_i-y_i)x_{ij} -\lambda w_j\right)
\]</span></p>
