---
title: "线性回归(Linear Regression)"
author: "Guangyao Zhao"
date: '2022-01-13T21:09:12-05:00'
image: ML.jpg
categories: machine learning
tags:
- machine learning
- algorithm
---


## 基本形式

给定由$n$个样本，$m$个属性描述的示例$D=\{(x_1,y_1), (x_2,y_2), (x_3,y_3), ..., (x_n,y_n)\}$，$i$表示第$i$个样本值。其中$x_i \in \mathbb{R}^m$，每个样本又包含$m$个维度，即$x_i=\{x_{i1},x_{i1},...,x_{im}\}$；$y \in \mathbb{R}$。

## 非矩阵形式

线性模型试图学习到一个通过属性的线性组合来预测的函数，即：
$$
\begin{aligned}
f(x) &=w_1x_1+w_2x_2+w_3x_3+...+w_mx_m+d\\
&=w^\mathrm{T}x+b
\end{aligned}
$$
其中：$w=\{w_1;w_2;w_3;...;w_m\}$，$x=\{x_1;x_2;x_3;...;x_m\}$。

线性回归试图寻求：$\left(w^{*}, b^{*}\right)=\arg \min \sum_{i=1}^{n}\left(f\left(x_{i}\right)-y_{i}\right)^{2}$。求解$w$和$b$使得均方误差最小的过程，称之为线性回归模型的最小二乘参数估计（parameter estimation），可将其求导得到： 

$$
  \begin{aligned}
    \frac{\partial E_{(w, b)}}{\partial w} &=2\left( \sum_{i=1}^{n} w^\mathrm{T} x_{i}^2-\sum_{i=1}^{n}\left(y_{i}-b\right) x_{i}\right) \\
    \frac{\partial E_{(w, b)}}{\partial b} &=2\left(n b-\sum_{i=1}^{n}\left(y_{i}-w^\mathrm{T} x_{i}\right)\right)
  \end{aligned}
$$

然后让两者为0 得到$w$和$b$的最优解的闭式（closed-form）解（注意：此处并未成功推导出$w$）： 

$$
  \begin{aligned}
    w&=\frac{\sum_{i=1}^{n} y_{i}\left(x_{i}-\bar{x}\right)}{\sum_{i=1}^{n} x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n} x_{i}\right)^{2}} \\
    b&=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-w^\mathrm{T} x_{i}\right)
  \end{aligned}
$$
其中$\bar{x}=\frac{1}{m}\sum_{i=1}^{m}x_i$。


## 矩阵形式

$w$的表示如下：

$$
w=\{w_1;w_2;w_3,...,w_m\}
$$

$\mathbf{X}$的表示如下：

$$
    \mathbf{X}=\left(\begin{array}{ccccc}
    x_{11} & x_{12} & \ldots & x_{1 m} & 1 \\
    x_{21} & x_{22} & \ldots & x_{2 m} & 1 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    x_{n 1} & x_{n 2} & \ldots & x_{n m} & 1
    \end{array}\right)=\left(\begin{array}{cc}
    \boldsymbol{x}_{1} & 1 \\
    \boldsymbol{x}_{2} & 1 \\
    \vdots & \vdots \\
    \boldsymbol{x}_{n} & 1
    \end{array}\right)
$$

$y$的的标记如下：

$$
y_i=\{y_1;y_2;y_3,...,y_n\}
$$

目标函数的推导如下：

$$
\begin{aligned}
\mathrm{L}(w) &=\sum_{i=1}^{n}\Vert w^\mathrm{T}x_i-y_i \Vert^2\\
&= (w^\mathrm{T}x_1-y_1, w^\mathrm{T}x_2-y_2,..., w^\mathrm{T}x_n-y_n)\begin{pmatrix}
 w^\mathrm{T}x_1-y_1
 \\w^\mathrm{T}x_2-y_2
 \\\vdots
\\w^\mathrm{T}x_n-y_n
\end{pmatrix}\\
&=\left [   \left (w^\mathrm{T}x_1, w^\mathrm{T}x_2,...,w^\mathrm{T}x_n \right ) - \left ( y_1,y_2,...,y_n \right ) \right ]
\left [ \begin{pmatrix}
 x_1
 \\x_2
 \\\vdots
\\x_n
\end{pmatrix}w-\begin{pmatrix}
 y_1
 \\y_2
 \\\vdots
\\y_n
\end{pmatrix} \right ] \\
&=\left ( w^\mathrm{T}\mathbf{X}^\mathrm{T}-y^\mathrm{T} \right ) \left ( \mathbf{X}w-y \right ) \\
&=\left ( \mathbf{X}w-y \right )^\mathrm{T} \left ( \mathbf{X}w-y \right ) 
\end{aligned}
$$

则，目标函数为：

$$
\underset{w}{\mathrm{argmin}}\,(\mathbf{X}w-y)^\mathrm{T}(\mathbf{X}w-y)
$$

对于目标函数：

$$
\begin{aligned}
\mathrm{L}(w) &=\left ( \mathbf{X}w-y \right )^\mathrm{T} \left ( \mathbf{X}w-y \right ) \\
&= w^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}w - w^\mathrm{T}\mathbf{X}^\mathrm{T}y-y^\mathrm{T}\mathbf{X}w+y^\mathrm{T}y\\
& =w^\mathrm{T}\mathbf{X}^\mathrm{T}\mathbf{X}w - 2w^\mathrm{T}\mathbf{X}^\mathrm{T}y+y^\mathrm{T}y  
\end{aligned}
$$

对上式（即目标函数）求导：

$$
\frac{\partial\, \mathrm{L}(w)}{\partial\, w} = 2\mathbf{X} ^\mathrm{T}\mathbf{X}w-2\mathbf{X}^\mathrm{T}y=0
$$

求解可得到解析解：$w=\left ( \mathbf{X}^\mathrm{T}\mathbf{X} \right )^{-1}\mathbf{X}^\mathrm{T}y$。在进行梯度下降的时候也是利用该公式，$w$的更新公式为：

$$
w := w-\alpha \mathbf{X} ^\mathrm{T}\left( \mathbf{X}w-y\right)
$$

其中$\alpha$为学习率。

## 几何解释
### 第一种几何解释

第一种几何解释很直观，它将误差分给了每一个样本。就是预测值和真实值之差的平方和。


### 第二种几何解释

第二种几何解释很新颖，它将误差分给了$m$个维度上。将样本$x_i$看作一个系数向量，即：

$$
f(w)=w^\mathrm{T}x_i=x_i^\mathrm{T}\beta
$$

其中$w$和$\beta$是未知的。由下图可知，当向量$y_i$垂直于$x_i$所形成的空间时（即投影），距离最小，即误差最小：

$$
\mathbf{X}^\mathrm{T}\left ( y-\mathbf{X}\beta \right ) =0\cdot\begin{pmatrix}
 0\\
 0 \\
 \vdots\\
 0
\end{pmatrix}_m
$$

求解得：

$$
\begin{aligned}
\mathbf{X}^\mathrm{T}y &= \mathbf{X}^\mathrm{T}\mathbf{X}\beta\\
\beta &= \left ( \mathbf{X}^\mathrm{T}\mathbf{X} \right ) ^{-1}\mathbf{X}^\mathrm{T}y
\end{aligned}
$$

此结果和最小二乘法的结果一致。


## 代码


```{python message = FALSE}
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_boston


def load_dataset():
    X, y = load_boston(return_X_y=True)
    return X, y


def normalize(X):
    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)


def cost_function(X, y, theta):
    n = X.shape[0]
    y_pre = h(X, theta)  # (m, 1)
    return float(np.matmul((y - y_pre).T, (y - y_pre)) / (2 * n))


def h(X, theta):
    return np.matmul(X, theta)


def gradient(X, y, theta, learning_rate=0.01, num=600):
    J_all = []  # 存放 loss
    n = X.shape[0]
    for i in range(num):
        y_pre = h(X, theta)
        cost_ = np.matmul(X.T, y_pre - y) / n  # 重点
        theta = theta - learning_rate * cost_
        J_all.append(cost_function(X, y, theta))
    return theta, J_all


def plot_cost(J_all):
    plt.xlabel('num')
    plt.ylabel('cost')
    plt.plot(J_all)
    plt.show()


# ===========
X, y = load_dataset()  # 加载数据集
X = np.array(X)
y = np.array(y).reshape((y.size, 1))

X = normalize(X)  # 标准化
X = np.hstack((X, np.ones((X.shape[0], 1))))  # 变化X
theta = np.ones((X.shape[1], 1))  # 初始化theta
learning_rate = 0.1
num = 200

theta, J_all = gradient(X, y, theta, learning_rate, num)
plot_cost(J_all)
```
